{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from collections import defaultdict, Counter\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "load_dotenv()\n",
    "\n",
    "WBPROJECT = os.getenv(\"WBPROJECT\")\n",
    "WBENTITIY = os.getenv(\"WBENTITIY\")\n",
    "INDIVIDUAL_RUNS_PATH = os.getenv(\"INDIVIDUAL_RUNS_PATH\")\n",
    "QUERIES_PATH = os.getenv(\"QUERIES_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(file_path):\n",
    "    with open(file_path, 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            yield json.loads(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions:\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Extract base domain\n",
    "def extract_base_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Remove subdomains (e.g., www.)\n",
    "    base_domain = parsed_url.netloc.split(\":\")[0]  # Removes any port if present\n",
    "    return base_domain\n",
    "\n",
    "# Count unique websites from visited URLs\n",
    "def count_unique_websites(visited_urls):\n",
    "    domain_counts = defaultdict(int)\n",
    "    for url, data in visited_urls.items():\n",
    "        visits = data.get(\"visits\", 0)\n",
    "        base_domain = extract_base_domain(url)\n",
    "        if base_domain:\n",
    "            domain_counts[base_domain] += visits\n",
    "    return domain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(log_file_path):\n",
    "    metrics = {\n",
    "        \"query\": None,  # To store the query from the UserProxy\n",
    "        \"final_answer\": None,  # To store the final answer from the Orchestrator\n",
    "        \"difficulty\": None,  # To store difficulty level\n",
    "        \"id_query\": None,  # To store query ID\n",
    "        \"query_num_listings\": None,  # To store number of requested listings in the query\n",
    "        \"query_num_websites\": None,  # To store number of requested websites in the query\n",
    "        \"num_webpage_visits\": None, #DONE\n",
    "        \"model_calls\": 0, #DONE\n",
    "        \"input_tokens\": 0, #DONE\n",
    "        \"output_tokens\": 0, #DONE\n",
    "        \"total_tokens\": 0, #DONE\n",
    "        \"screenshots\": 0, #DONE\n",
    "        \"errors\": 0, \n",
    "        \"failed_action_attempts\": None, #TODO\n",
    "        \"visited_urls\": {},\n",
    "        \"domain_counts\" : {}, #DONE\n",
    "        # postprocessing\n",
    "        \"success\": 0, \n",
    "        \"complete_fulfilment_ratio\": None,\n",
    "        \"requirement_score\": None,\n",
    "        \"number_listings\": None,\n",
    "        \"website_coverage\": None,\n",
    "        \"num_type_search_engine\": 0, #DONE\n",
    "        \"num_type_immo_website\": 0, #DONE\n",
    "        \"num_queries_search_engine\": 0, #DONE\n",
    "        \"num_queries_immo_website\": 0, #DONE\n",
    "        \"primary_failure\": None,\n",
    "        \"secondary_failure\": None,\n",
    "        \"folder_name\": None, #DONE\n",
    "        \"timestamp\": None,\n",
    "        \"model_name\": \"magentic\",\n",
    "        \"text_model\": 1,\n",
    "        \"vision_model\": 1,\n",
    "        \"multi_agent\": 1\n",
    "    }\n",
    "\n",
    "    search_engine_domains = {\"www.bing.com\", \"www.google.com\"}\n",
    "\n",
    "    for log in parse_log_file(log_file_path):\n",
    "        # Extract query from UserProxy\n",
    "        if log.get(\"source\") == \"UserProxy\" and \"message\" in log:\n",
    "            metrics[\"query\"] = log[\"message\"]\n",
    "\n",
    "        # Extract final answer from Orchestrator\n",
    "        if log.get(\"source\") == \"Orchestrator (final answer)\" and \"message\" in log:\n",
    "            metrics[\"final_answer\"] = log[\"message\"]\n",
    "\n",
    "        # Extract initialization arguments\n",
    "        if log.get(\"type\") == \"Initialization\" and \"arguments\" in log:\n",
    "            args = log[\"arguments\"]\n",
    "            metrics[\"difficulty\"] = args.get(\"difficulty\", None)\n",
    "            metrics[\"id_query\"] = args.get(\"id_query\", None)\n",
    "            metrics[\"query_num_listings\"] = args.get(\"query_num_listings\", None)\n",
    "            metrics[\"query_num_websites\"] = args.get(\"query_num_websites\", None)\n",
    "            folder_name = args.get(\"folder_name\", None)\n",
    "            metrics[\"folder_name\"] = folder_name \n",
    "\n",
    "        if log.get(\"type\") == \"WebSurferEvent\" and \"url\" in log:\n",
    "            url = log[\"url\"]\n",
    "            action = log.get(\"action\", None)\n",
    "            \n",
    "            # Extract domain for this URL\n",
    "            domain = extract_base_domain(url)\n",
    "\n",
    "            # Initialize the URL entry if not already present\n",
    "            if url not in metrics[\"visited_urls\"]:\n",
    "                metrics[\"visited_urls\"][url] = {\"visits\": 0, \"clicking\": 0, \"typing\": 0}\n",
    "\n",
    "            # Increment visits\n",
    "            metrics[\"visited_urls\"][url][\"visits\"] += 1\n",
    "\n",
    "            # Increment specific action counters\n",
    "            if action == \"click\":\n",
    "                metrics[\"visited_urls\"][url][\"clicking\"] += 1\n",
    "            elif action == \"web_search\":\n",
    "                metrics[\"visited_urls\"][url][\"typing\"] += 1\n",
    "\n",
    "                # Count search engine queries\n",
    "                if domain in search_engine_domains:\n",
    "                    metrics[\"num_queries_search_engine\"] += 1\n",
    "                else:  # Assume non-search-engine searches are real estate queries\n",
    "                    metrics[\"num_queries_immo_website\"] += 1\n",
    "\n",
    "\n",
    "            if \"screenshot\" in log.get(\"message\", \"\").lower():\n",
    "                metrics[\"screenshots\"] += 1\n",
    "\n",
    "        elif log.get(\"type\") == \"LLMCallEvent\":\n",
    "            metrics[\"model_calls\"] += 1\n",
    "            metrics[\"input_tokens\"] += log.get(\"prompt_tokens\", 0)\n",
    "            metrics[\"output_tokens\"] += log.get(\"completion_tokens\", 0)\n",
    "\n",
    "        elif \"error\" in log.get(\"message\", \"\").lower():\n",
    "            metrics[\"errors\"] += 1\n",
    "\n",
    "\n",
    "    # Calculate derived metrics\n",
    "    metrics[\"total_tokens\"] = metrics[\"input_tokens\"] + metrics[\"output_tokens\"]\n",
    "    metrics[\"num_type_search_engine\"] = metrics[\"num_queries_search_engine\"]\n",
    "    metrics[\"num_type_immo_website\"] = metrics[\"num_queries_immo_website\"]\n",
    "    metrics[\"num_webpage_visits\"] = len(metrics[\"visited_urls\"])\n",
    "    metrics[\"domain_counts\"] = count_unique_websites(metrics[\"visited_urls\"])\n",
    "    metrics[\"timestamp\"] = re.search(r'(\\d{14})', folder_name).group(1)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"../logs/archive/vision_magnetic_TEST_02_20241215154814/log.jsonl\"\n",
    "file_path = \"../logs/vision_magnetic_advanced_02_20241217204927/log.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = extract_metrics(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual updates metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the root directory path to sys.path\n",
    "sys.path.append(QUERIES_PATH )\n",
    "from queries.failure_reasons import FAILURE_REASONS, modify_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_save_metrics(metrics, updates):\n",
    "    \"\"\"\n",
    "    Updates the metrics dictionary with the given updates and saves it as 'metrics.json' \n",
    "    in the folder specified by the 'folder_name' key in metrics.\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): The extracted metrics to update.\n",
    "        updates (dict): A dictionary containing the new values for postprocessing metrics.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the updated metrics file.\n",
    "    \"\"\"\n",
    "    # Update the metrics with the new values\n",
    "    for key, value in updates.items():\n",
    "        metrics[key] = value\n",
    "\n",
    "    # Ensure 'folder_name' is in the metrics\n",
    "    if \"folder_name\" not in metrics or not metrics[\"folder_name\"]:\n",
    "        raise ValueError(\"The metrics dictionary must contain a valid 'folder_name' key.\")\n",
    "\n",
    "    # Extract the folder name from metrics\n",
    "    output_folder = metrics[\"folder_name\"]\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define the output path for the updated metrics.json\n",
    "    output_path = os.path.join(output_folder, \"metrics.json\")\n",
    "\n",
    "    # Write the updated metrics to the file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Updated metrics saved to: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(FAILURE_REASONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New values for the postprocessing metrics\n",
    "updates = {\n",
    "    \"success\": 0,\n",
    "    \"primary_failure\": 'token_limitation',\n",
    "    \"secondary_failure\": ['captcha'],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Modify the metrics\n",
    "print(\"Metrics before update:\", metrics)\n",
    "update_and_save_metrics(metrics, updates)\n",
    "print(\"Updated Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dataframe_to_wandb(df, project, entity, artifact_name=\"experiment_metrics\", artifact_type=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Log a Pandas DataFrame to W&B, ensuring dictionaries remain in single cells.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The Pandas DataFrame to log.\n",
    "        project (str): W&B project name.\n",
    "        entity (str): W&B entity name.\n",
    "        artifact_name (str): Name of the W&B artifact.\n",
    "        artifact_type (str): Type of the W&B artifact.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Serialize dictionary columns to JSON strings\n",
    "    for col in df.columns:\n",
    "        if any(isinstance(value, dict) for value in df[col]):\n",
    "            df[col] = df[col].apply(json.dumps)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=project, entity=entity)\n",
    "\n",
    "    # Define W&B table columns explicitly\n",
    "    columns = df.columns.tolist()\n",
    "    wandb_table = wandb.Table(columns=columns)\n",
    "\n",
    "    # Add rows to the W&B table\n",
    "    for _, row in df.iterrows():\n",
    "        wandb_table.add_data(*row)\n",
    "\n",
    "    # Log the table\n",
    "    wandb.log({\"Experiment Metrics\": wandb_table})\n",
    "\n",
    "    # Finish the W&B run\n",
    "    wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def display_metrics_as_table(metrics):\n",
    "    \"\"\"\n",
    "    Display metrics in a tabular format using Pandas.\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Dictionary of metrics from a single experiment.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame representation of the metrics.\n",
    "    \"\"\"\n",
    "    # Convert the metrics dictionary to a DataFrame\n",
    "    table = pd.DataFrame([metrics])  # Wrap metrics in a list to create a single-row DataFrame\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = display_metrics_as_table(metrics)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dataframe_to_wandb(\n",
    "    df,\n",
    "    project=WBPROJECT,\n",
    "    entity=WBENTITIY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "df_combined = pd.read_csv(INDIVIDUAL_RUNS_PATH, sep=\";\") #TODO\n",
    "df_combined.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.astype(object)\n",
    "df = df.astype(object)\n",
    "df_combined = pd.concat([df_combined, df], ignore_index=True)\n",
    "# save to csv\n",
    "df_combined.to_csv(INDIVIDUAL_RUNS_PATH, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load csv\n",
    "# df= pd.read_csv(INDIVIDUAL_RUNS_PATH, sep=\";\") #TODO\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename columns: \n",
    "# \"\"\" \n",
    "# ID_query -> id_query\n",
    "# num_listings -> query_num_listings\n",
    "# num_pages -> query_num_websites\n",
    "# llm_calls -> model_calls\n",
    "# no_action -> failed_action_attempts\n",
    "# \"\"\"\n",
    "# df.rename(columns={\n",
    "#     \"ID_query\": \"id_query\",\n",
    "#     \"num_listings\": \"query_num_listings\",\n",
    "#     \"num_pages\": \"query_num_websites\",\n",
    "#     \"llm_calls\": \"model_calls\",\n",
    "#     \"no_action\": \"failed_action_attempts\"\n",
    "# }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using 'foldername', extract timestamp and save as 'timestamp' column\n",
    "# df['timestamp'] = df['folder_name'].str.extract(r'_(\\d{14})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to csv\n",
    "# df.to_csv(INDIVIDUAL_RUNS_PATH, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
